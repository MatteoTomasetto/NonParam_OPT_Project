rm(list=ls())
library(LearnBayes)
library(rjags)   # to interface R with JAGS
library(pscl)
data(absentee)
#help(absentee)
summary(absentee)
install(rjags)
install.packages(rjags)
install.package(rjags)
modelRegress=jags.model("regression.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
#############################################
## A (CONJUGATE) linear regression example ##
## Example 2.15 - S. Jackman (2009)        ##
#############################################
## In November 1993, the state of Pennsylvania conducted elections
## for its state legislature. The result in the Senate election
## in the 2nd district (based in Philadelphia) was challenged
## in court, and ultimately overturned. The Democratic candidate
## won 19,127 of the votes cast by voting machine, while the
## Republican won 19,691 votes cast by voting machine, giving
## the Republican a lead of 564 votes. However, the Democrat won
## 1,396 absentee ballots, while the Republican won just 371 absentee
## ballots, more than offsetting the Republican lead based
## on the votes recorded by machines on election day.
## The Republican candidate sued, claiming that many of the
## absentee ballots were fraudulent.
## The judge in the case solicited expert analysis from
## Orley Ashenfelter, an economist at Princeton University.
## Ashenfelter examined the relationship between absentee vote
## margins and machine vote margins in 21 previous
## Pennsylvania Senate elections in seven districts in the
## Philadelphia area over the preceding decade.
## OUTLIER detection problem in the contect of simple linear
## regression in the Pennsylvania state senate election
## Response Y= abseentee vote margins (difference of
##             Democrat percentage and Republican percentage)
##             in the absentee ballot
## Absentee ballot = VOTO A DISTANZA
## See Jackman, from p. 87
## COVARIATE x = machine vote margins (difference of Democrat
##              percentage and Republican percentage)
## R package pscl where you can find the dataset
library(pscl)
data(absentee)
help(absentee) # A data frame with 22 observations on 8 variables.
summary(absentee)
attach(absentee)
## create variables for regression analysis
y <- (absdem - absrep)/(absdem + absrep)*100
x <- (machdem - machrep)/(machdem + machrep)*100
x[22]
y[22]
# The 22nd datapoint is suspect!
## DATA PLOT ##
x11()
plot(y~x,type="n",xlim=range(x),ylim=range(y),
xlab="Democratic Margin, Machine Ballots (Percentage Points)",
ylab="Democratic Margin, Absentee Ballots (Percentage Points)")
ols <- lm(y ~ x, subset=c(rep(TRUE,21),FALSE)) ## OLS analysis of the
## dataset, but DROP data point 22
abline(ols,lwd=2) ## overlay ols (retta di regressione stimata, senza il dato 22)
points(x[-22],y[-22],pch=1) ## data
points(x[22],y[22],pch=16) ## disputed data point
text(x[22],y[22],"Disputed\nElection",cex=.75,adj=1.25)
axis(1)
axis(2)
summary(ols)
############## DATA ANALYSIS WITHOUT the 22nd observation
y=y[1:21]
x=x[1:21]
X=as.matrix(cbind(rep(1,21),x))
n=length(y)
k=dim(X)[2] #lo abbiamo chiamato p alla lavagna!
######## PARAMETERS
## beta0, beta1, sigma^2
#### ORDINARY LEAST SQUARE ESTIMATION OF beta
inv=function(X)
{
# RETURN THE INVERSE OF THE SYMMETRIC MATRIX X
EV=eigen(X)
EV$vector%*%diag(1/EV$values)%*%t(EV$vector)
}
inv(t(X)%*%X); solve(t(X)%*%X)#use either these commands to compute the inverse of a square matrix
betahat=inv(t(X)%*%X)%*%t(X)%*%y
betahat
#### FREQUENTIST UNBIASED ESTIMATION OF sigma2
S2=t(y-X%*%betahat)%*%(y-X%*%betahat) ## SOMMA dei RESIDUI al quadrato
sigma2hat=S2/(n-k)
sigma2hat
sqrt(sigma2hat)
#### ESTIMATION OF THE VARIANCE OF betahat
c(sigma2hat)*diag(inv(t(X)%*%X))
#### ESTIMATION OF THE sd OF betahat
sqrt(c(sigma2hat)*diag(inv(t(X)%*%X)))
###########################################################################################
################################################
###                 JAGS use                 ###
###         Just Another Gibbs Sampler       ###
###           Alessandra Guglielmi           ###
################################################
##    http://mcmc-jags.sourceforge.net/
##    WEB SITE: info and instructions on JAGS downloading
################################################
## JAGS is Just Another Gibbs Sampler. It is a program for the analysis of Bayesian models
## using Markov Chain Monte Carlo (MCMC) which is not wholly unlike
## OpenBUGS (http://www.openbugs.info).
## JAGS was written with three aims in mind: to have an engine for the BUGS language that runs on Unix;
## to be extensible, allowing users to write their own functions, distributions, and samplers;
## and to be a platform for experimentation with ideas in Bayesian modelling.
## JAGS is designed to work closely with the R language and environment for statistical
## computation and graphics (http://www.r-project.org).
## You will find it useful to install the coda package for R to analyze the output.
## You can also use the rjags package to work directly with JAGS from within R.
rm(list=ls())
########## How to call JAGS from R? ##########
##########
#Load the library
setwd("C:/LocalDiskAlessandra/Doculavoro/Didattica/StatisticaBayesiana/mat2122/CODES")
library(rjags)   # to interface R with JAGS
library(pscl)
data(absentee)
#help(absentee)
summary(absentee)
attach(absentee)
## create variables for regression analysis
y <- (absdem - absrep)/(absdem + absrep)*100
x <- (machdem - machrep)/(machdem + machrep)*100
x.sosp=x[22]
y.sosp=y[22]
y=y[1:21]
x=x[1:21]
#### 21 (bivariate) data points
## JAGS takes a user's description of a Bayesian model for data,
## and returns an MCMC sample of the posterior distribution
## Define the data (in this case, datapoints (y_i), the covariate
##  vector (x_i), the sample size,
##                   the new covariate value xstar) in a list
data = list(y=y[1:21],
x=x[1:21],
n=21,
xstar=x.sosp)
## A list of initial value for the MCMC algorithm
# that JAGS will implement
inits = function() {list(beta=c(0,0),
sigma=5,
ystar=0) }
## JAGS can automatically inizialize the chain, but the efficiency
## of the MCMC can be improved if we intelligently provide
## reasonable starting values, i.e. values in the midst of the
## posterior, e.g. MLE, or values close to the MLE
## The Bayesian model is in the text file "regression.bug". This
## file, in addition to the list "data", is taken as an input
## by JAGS for generating the MCMC in 3 steps.
## The first step (jags.model) gets all the info into JAGS and
## let JAGS figure out appropriate sampler for the model.
## The second step (update) runs the chain for a burn-in period.
## The third step (coda.samples) runs and records the MCMC sample
## we will subsequently examine (using R).
modelRegress=jags.model("regression.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
install.packages("rjags")
install.packages("coda")
library(rjags)   # to interface R with JAGS
library(pscl)
data(absentee)
#help(absentee)
summary(absentee)
attach(absentee)
y <- (absdem - absrep)/(absdem + absrep)*100
x <- (machdem - machrep)/(machdem + machrep)*100
x.sosp=x[22]
y.sosp=y[22]
y=y[1:21]
x=x[1:21]
#############################################
## A (CONJUGATE) linear regression example ##
## Example 2.15 - S. Jackman (2009)        ##
#############################################
## In November 1993, the state of Pennsylvania conducted elections
## for its state legislature. The result in the Senate election
## in the 2nd district (based in Philadelphia) was challenged
## in court, and ultimately overturned. The Democratic candidate
## won 19,127 of the votes cast by voting machine, while the
## Republican won 19,691 votes cast by voting machine, giving
## the Republican a lead of 564 votes. However, the Democrat won
## 1,396 absentee ballots, while the Republican won just 371 absentee
## ballots, more than offsetting the Republican lead based
## on the votes recorded by machines on election day.
## The Republican candidate sued, claiming that many of the
## absentee ballots were fraudulent.
## The judge in the case solicited expert analysis from
## Orley Ashenfelter, an economist at Princeton University.
## Ashenfelter examined the relationship between absentee vote
## margins and machine vote margins in 21 previous
## Pennsylvania Senate elections in seven districts in the
## Philadelphia area over the preceding decade.
## OUTLIER detection problem in the contect of simple linear
## regression in the Pennsylvania state senate election
## Response Y= abseentee vote margins (difference of
##             Democrat percentage and Republican percentage)
##             in the absentee ballot
## Absentee ballot = VOTO A DISTANZA
## See Jackman, from p. 87
## COVARIATE x = machine vote margins (difference of Democrat
##              percentage and Republican percentage)
## R package pscl where you can find the dataset
library(pscl)
data(absentee)
help(absentee) # A data frame with 22 observations on 8 variables.
summary(absentee)
attach(absentee)
## create variables for regression analysis
y <- (absdem - absrep)/(absdem + absrep)*100
x <- (machdem - machrep)/(machdem + machrep)*100
x[22]
y[22]
# The 22nd datapoint is suspect!
## DATA PLOT ##
x11()
plot(y~x,type="n",xlim=range(x),ylim=range(y),
xlab="Democratic Margin, Machine Ballots (Percentage Points)",
ylab="Democratic Margin, Absentee Ballots (Percentage Points)")
ols <- lm(y ~ x, subset=c(rep(TRUE,21),FALSE)) ## OLS analysis of the
## dataset, but DROP data point 22
abline(ols,lwd=2) ## overlay ols (retta di regressione stimata, senza il dato 22)
points(x[-22],y[-22],pch=1) ## data
points(x[22],y[22],pch=16) ## disputed data point
text(x[22],y[22],"Disputed\nElection",cex=.75,adj=1.25)
axis(1)
axis(2)
summary(ols)
############## DATA ANALYSIS WITHOUT the 22nd observation
y=y[1:21]
x=x[1:21]
X=as.matrix(cbind(rep(1,21),x))
n=length(y)
k=dim(X)[2] #lo abbiamo chiamato p alla lavagna!
######## PARAMETERS
## beta0, beta1, sigma^2
#### ORDINARY LEAST SQUARE ESTIMATION OF beta
inv=function(X)
{
# RETURN THE INVERSE OF THE SYMMETRIC MATRIX X
EV=eigen(X)
EV$vector%*%diag(1/EV$values)%*%t(EV$vector)
}
inv(t(X)%*%X); solve(t(X)%*%X)#use either these commands to compute the inverse of a square matrix
betahat=inv(t(X)%*%X)%*%t(X)%*%y
betahat
#### FREQUENTIST UNBIASED ESTIMATION OF sigma2
S2=t(y-X%*%betahat)%*%(y-X%*%betahat) ## SOMMA dei RESIDUI al quadrato
sigma2hat=S2/(n-k)
sigma2hat
sqrt(sigma2hat)
#### ESTIMATION OF THE VARIANCE OF betahat
c(sigma2hat)*diag(inv(t(X)%*%X))
#### ESTIMATION OF THE sd OF betahat
sqrt(c(sigma2hat)*diag(inv(t(X)%*%X)))
###########################################################################################
################################################
###                 JAGS use                 ###
###         Just Another Gibbs Sampler       ###
###           Alessandra Guglielmi           ###
################################################
##    http://mcmc-jags.sourceforge.net/
##    WEB SITE: info and instructions on JAGS downloading
################################################
## JAGS is Just Another Gibbs Sampler. It is a program for the analysis of Bayesian models
## using Markov Chain Monte Carlo (MCMC) which is not wholly unlike
## OpenBUGS (http://www.openbugs.info).
## JAGS was written with three aims in mind: to have an engine for the BUGS language that runs on Unix;
## to be extensible, allowing users to write their own functions, distributions, and samplers;
## and to be a platform for experimentation with ideas in Bayesian modelling.
## JAGS is designed to work closely with the R language and environment for statistical
## computation and graphics (http://www.r-project.org).
## You will find it useful to install the coda package for R to analyze the output.
## You can also use the rjags package to work directly with JAGS from within R.
rm(list=ls())
########## How to call JAGS from R? ##########
library(rjags)   # to interface R with JAGS
library(pscl)
data(absentee)
#help(absentee)
summary(absentee)
library(rjags)   # to interface R with JAGS
library(pscl)
install.packages("pscl")
library(rjags)   # to interface R with JAGS
library(pscl)
data(absentee)
#help(absentee)
summary(absentee)
attach(absentee)
## create variables for regression analysis
y <- (absdem - absrep)/(absdem + absrep)*100
x <- (machdem - machrep)/(machdem + machrep)*100
x.sosp=x[22]
y.sosp=y[22]
y=y[1:21]
x=x[1:21]
data = list(y=y[1:21],
x=x[1:21],
n=21,
xstar=x.sosp)
## A list of initial value for the MCMC algorithm
# that JAGS will implement
inits = function() {list(beta=c(0,0),
sigma=5,
ystar=0) }
modelRegress=jags.model("regression.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
df
install.packages("medicaldata")
library(medicaldata)
df <- medicaldata::opt
df[:,1]
df[,1]
df[,2]
df[,3]
df[,13]
df[,14]
df[,15]
df[,16]
df[,19]
df[,18]
df[,19]
df[,20]
df[,23]
df[,25]
df[,24]
df[,28]
df[,29]
df[,30]
df[,33]
df[,34]
df[,35]
df[,36]
df[,35]
sum(is.na(df[,35]))
sum(is.na(df[,36]))
sum(is.na(df[,49]))
df[,68]
sum(is.na(df[,68]))
pairs(df[,37],df[,68])
pairs(df[,c(37,68)])
df[,68]
df[,37]
df[,68]
df[,69]
df[,68]
df[,69]
df[,70]
df[,71]
df[,74]
df[,78]
df[,101]
df[,104]
df[,105]
df[,139]
sum(is.na(df[,139]))
df[,69]
install.packages("medicaldata")
library(medicaldata)
df <- medicaldata::opt
pairs(df[,c(37,68)])
df[,69]
df[,70]
df[,71]
df[,70]
idx_lost_to_follow_up = which(df[,70] == "")
idx_lost_to_follow_up
idx_lost_to_follow_up = which(df[,70] == " ")
idx_lost_to_follow_up
df[11,70]
# idx_out <- [19, 34, 36]
names(df)
idx_lost_to_follow_up = which(df[,70] == "Yes")
idx_lost_to_follow_up
idx_lost_to_follow_up = which(df[,70] == "")
idx_lost_to_follow_up
is(df[11,70])
df[11,70]==""
df[11,70]==" "
df[11,70]== NA
df[:,70]
df[,70]
# idx_out <- [19, 34, 36]
df[,69]
idx_NA = which(df[,70] != "Yes" & df[,70] != "No")
idx_NA
idx_NA = which(df[,70] != "Yes" || df[,70] != "No")
idx_NA
idx_NA = which(df[,70] != "Yes" || "No")
idx_NA
# idx_out <- [19, 34, 36]
idx_NA = which(df[,70] == "")
idx_NA
idx_NA = which(df[,70] == factor(""))
df[,70]
levels(df[,70])[1]
levels(df[,70])[2]
# idx_out <- [19, 34, 36]
idx_NA = which(df[,70] == levels(df[,70])[1])
idx_NA
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
cmdstanr::install_cmdstan()
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
cmdstanr::install_cmdstan()
library(cmdstanr)
check_cmdstan_toolchain(fix=TRUE)
cmdstanr::install_cmdstan()
install.packages(RTools)
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
library(cmdstanr)
check_cmdstan_toolchain(fix=TRUE)
cmdstanr::install_cmdstan()
check_cmdstan_toolchain(fix=TRUE)
check_cmdstan_toolchain(fix = TRUE)
install_cmdstan()
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
library(cmdstanr)
check_cmdstan_toolchain(fix=TRUE)
library(cmdstanr)
check_cmdstan_toolchain(fix=TRUE)
cmdstanr::install_cmdstan()
library(medicaldata)
df <- medicaldata::opt
setwd("C:/Users/matte/Desktop/NonParam_OPT_Project")
#setwd("C:/Users/asia/Desktop/NonParam_OPT_Project")
#setwd("C:/Users/laura/Desktop/NonParam_OPT_Project")
source("DATA PREPROCESSING.R")
##############################################################################
################################ FUNCTIONS ###################################
##############################################################################
library(MASS)
library(rgl)
library(DepthProc)
library(hexbin)
library(packagefinder)
library(aplpack)
library(robustbase)
perm_t_test_mean = function(x,y,iter=1e3){
T0=abs(mean(x)-mean(y))
T_stat=numeric(iter)
x_pooled=c(x,y)
n=length(x_pooled)
n1=length(x)
for(perm in 1:iter){
# permutation:
permutation <- sample(1:n)
x_perm <- x_pooled[permutation]
x1_perm <- x_perm[1:n1]
x2_perm <- x_perm[(n1+1):n]
# test statistic:
T_stat[perm] <- abs(mean(x1_perm) - mean(x2_perm))
}
# p-value
p_val <- sum(T_stat>=T0)/iter
return(p_val)
}
perm_t_test_median = function(x,y,iter=1e3){
T0=abs(median(x)-median(y))
T_stat=numeric(iter)
x_pooled=c(x,y)
n=length(x_pooled)
n1=length(x)
for(perm in 1:iter){
# permutation:
permutation <- sample(1:n)
x_perm <- x_pooled[permutation]
x1_perm <- x_perm[1:n1]
x2_perm <- x_perm[(n1+1):n]
# test statistic:
T_stat[perm] <- abs(median(x1_perm) - median(x2_perm))
}
# p-value
p_val <- sum(T_stat>=T0)/iter
return(p_val)
}
perm_t_test_prop = function(x,y,iter=1e3){
T0=abs(mean(as.numeric(x)-1)-mean(as.numeric(y)-1))
T_stat=numeric(iter)
x_pooled=c(x,y)
n=length(x_pooled)
n1=length(x)
for(perm in 1:iter){
# permutation:
permutation <- sample(1:n)
x_perm <- x_pooled[permutation]
x1_perm <- x_perm[1:n1]
x2_perm <- x_perm[(n1+1):n]
# test statistic:
T_stat[perm] <- abs(mean(as.numeric(x1_perm)-1) - mean(as.numeric(x2_perm)-1))
}
# p-value
p_val <- sum(T_stat>=T0)/iter
return(p_val)
}
idx_Var = c(71,72)
idx_NA = which(is.na(df[,idx_Var]))
if (length(idx_NA)==0){
Var_test = df[,idx_Var]
Group_test = df[,3]
x = Var_test[which(Group_test =='C'),]
y = Var_test[which(Group_test =='T'),]
} else {
Var_test = df[-idx_NA,idx_Var]
Group_test = df[-idx_NA,3]
x = Var_test[which(Group_test =='C'),]
y = Var_test[which(Group_test =='T'),]
}
depthMedian(x,depth_params = list(method='Tukey'))
